import random
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from mesa import Agent, Model
from mesa.time import BaseScheduler
from mesa.space import MultiGrid

############################
# The Disaster Model
############################

class DisasterModel(Model):
    def __init__(self, width=50, height=50, num_humans=75, num_ai=1, noise=0.1, exploratory_fraction=0.5):
        # Initialize the model.
        Model.__init__(self)
        self.width = width
        self.height = height
        self.num_humans = num_humans
        self.num_ai = num_ai
        self.noise = noise
        self.exploratory_fraction = exploratory_fraction

        # Set up scheduler and grid.
        self.schedule = BaseScheduler(self)
        self.grid = MultiGrid(width, height, torus=False)
        self.current_step = 0

        # Define the environment (a dict mapping (x,y) to disaster level).
        self.environment = {}
        for x in range(width):
            for y in range(height):
                self.environment[(x, y)] = 0
        self.init_disaster()

        # Global logging variables.
        self.info_calls_human = 0
        self.info_calls_ai = 0
        self.total_reward_this_step = 0

        self.trust_exploratory_human_over_time = []
        self.trust_exploratory_ai_over_time = []
        self.trust_exploitative_human_over_time = []
        self.trust_exploitative_ai_over_time = []
        self.reward_over_time = []
        self.info_calls_human_over_time = []
        self.info_calls_ai_over_time = []

        # Create human agents.
        self.human_agents = []
        for i in range(num_humans):
            pos = (random.randrange(width), random.randrange(height))
            # Set strategy according to exploratory_fraction.
            strategy = "exploratory" if i < int(num_humans * exploratory_fraction) else "exploitative"
            human = HumanAgent(i, self, pos, strategy)
            self.human_agents.append(human)
            self.schedule.add(human)
            self.grid.place_agent(human, pos)

        # Create AI agents.
        self.ai_agents = []
        for i in range(num_ai):
            pos = (random.randrange(width), random.randrange(height))
            # For simplicity, set AI strategy to exploitative here.
            ai = AIAgent(num_humans + i, self, strategy="exploitative", epsilon=0.05)
            self.ai_agents.append(ai)
            self.schedule.add(ai)
            # Optionally, you can place AI agents on the grid.
            # self.grid.place_agent(ai, pos)

        # Build a social network among human agents.
        self.create_social_network()

    def init_disaster(self):
        # Choose an epicenter at random.
        epicenter = (random.randrange(self.width), random.randrange(self.height))
        decay_factor = 5  # determines how quickly destruction decays.
        # Use Manhattan distance.
        for x in range(self.width):
            for y in range(self.height):
                dist = abs(x - epicenter[0]) + abs(y - epicenter[1])
                level = max(5 - (dist // decay_factor), 0)
                self.environment[(x, y)] = level

    def create_social_network(self):
        # Build a Watts-Strogatz small-world network.
        G = nx.watts_strogatz_graph(self.num_humans, k=4, p=0.3)
        for human in self.human_agents:
            human.social_neighbors = list(G.neighbors(human.unique_id))

    def update_environment(self):
        # A simple dynamic: cells next to a cell with level 5 intensify.
        new_env = self.environment.copy()
        for x in range(self.width):
            for y in range(self.height):
                neighbors = self.grid.get_neighborhood((x, y), moore=True, include_center=False)
                if any(self.environment.get(n, 0) == 5 for n in neighbors):
                    if new_env[(x, y)] < 5:
                        new_env[(x, y)] += 1
        self.environment = new_env

    def step(self):
        # Reset per-step counters.
        self.info_calls_human = 0
        self.info_calls_ai = 0
        self.total_reward_this_step = 0
        
        self.update_environment()
        self.schedule.step()
        self.current_step += 1
        
        # After agents have acted, log trust values.
        exp_human = []
        exp_ai = []
        expl_human = []
        expl_ai = []
        for agent in self.human_agents:
            human_trusts = [agent.trust[src] for src in agent.trust if src < self.num_humans]
            ai_trusts = [agent.trust[src] for src in agent.trust if src >= self.num_humans]
            if agent.strategy == "exploratory":
                if human_trusts:
                    exp_human.append(np.mean(human_trusts))
                if ai_trusts:
                    exp_ai.append(np.mean(ai_trusts))
            else:
                if human_trusts:
                    expl_human.append(np.mean(human_trusts))
                if ai_trusts:
                    expl_ai.append(np.mean(ai_trusts))
        self.trust_exploratory_human_over_time.append(np.mean(exp_human) if exp_human else 0)
        self.trust_exploratory_ai_over_time.append(np.mean(exp_ai) if exp_ai else 0)
        self.trust_exploitative_human_over_time.append(np.mean(expl_human) if expl_human else 0)
        self.trust_exploitative_ai_over_time.append(np.mean(expl_ai) if expl_ai else 0)
        
        # Log reward and info calls.
        self.reward_over_time.append(self.total_reward_this_step)
        self.info_calls_human_over_time.append(self.info_calls_human)
        self.info_calls_ai_over_time.append(self.info_calls_ai)

############################
# The Human Agent
############################

class HumanAgent(Agent):
    def __init__(self, unique_id, model, pos, strategy="exploratory"):
        Agent.__init__(self, model)
        self.unique_id = unique_id
        self.model = model
        self.pos = pos
        self.strategy = strategy  # "exploratory" or "exploitative"
        
        # Expectation for each cell.
        self.expectation = {(x, y): 0 for x in range(model.width) for y in range(model.height)}
        self.expectation[pos] = model.environment[pos]
        # Trust: initialize human sources at 0.7 and AI sources at 0.5.
        self.trust = {}
        # Memory of observed states.
        self.memory = {}
        # Social neighbors (set later by social network generator).
        self.social_neighbors = []
        self.noise = model.noise
        # Pending rewards: list of tuples (cell, decision_round, expected).
        self.pending_rewards = []
        # Total reward accumulated (for agent-level tracking if needed).
        self.reward = 0

    def step(self):
        self.sense_environment()
        self.request_information()
        target = self.decide_target()
        self.pending_rewards.append((target, self.model.current_step, self.expectation[target]))
        self.process_pending_rewards()

    def sense_environment(self):
        # Sense the Moore neighborhood (including own cell).
        neighbors = self.model.grid.get_neighborhood(self.pos, moore=True, include_center=True)
        for cell in neighbors:
            observed = self.model.environment.get(cell, 0)
            self.memory.setdefault(cell, []).append(observed)
            self.expectation[cell] = observed

    def request_information(self):
        sources = []
        # Request from up to 3 human neighbors.
        if self.social_neighbors:
            random.shuffle(self.social_neighbors)
            sources.extend(self.social_neighbors[:3])
        # Also request from one random AI agent.
        if self.model.ai_agents:
            ai_source = random.choice(self.model.ai_agents)
            sources.append(ai_source.unique_id)
        
        for source_id in sources:
            source = self.get_agent_by_id(source_id)
            if source:
                if isinstance(source, HumanAgent):
                    self.model.info_calls_human += 1
                    info = source.share_info()
                    # If the source is in a highly affected cell, it cannot respond.
                    if self.model.environment[source.pos] >= 3:
                        info = None
                    # With some noise, return an incorrect value.
                    if info is not None and random.random() < self.noise:
                        # Replace the destruction level with a random value between 0 and 5.
                        info = (info[0], random.randint(0, 5))
                    if info is not None:
                        cell, destruction = info
                        trust_level = self.trust.get(source_id, 0.7)
                        # Update expectation as a weighted average.
                        self.expectation[cell] = (self.expectation.get(cell, 0) + trust_level * destruction) / (1 + trust_level)
                    else:
                        self.trust[source_id] = max(0, self.trust.get(source_id, 0.7) - 0.05)
                else:  # Source is an AI.
                    self.model.info_calls_ai += 1
                    info = source.respond_info(self)
                    if info is not None:
                        cell, destruction = info
                        trust_level = self.trust.get(source_id, 0.5)
                        self.expectation[cell] = (self.expectation.get(cell, 0) + trust_level * destruction) / (1 + trust_level)

    def decide_target(self):
        # Choose the cell with the highest expected destruction.
        target = max(self.expectation.items(), key=lambda x: x[1])[0]
        return target

    def process_pending_rewards(self):
        new_pending = []
        for (cell, decision_round, expected) in self.pending_rewards:
            if self.model.current_step - decision_round >= 2:
                actual = self.model.environment.get(cell, 0)
                reward = 0
                if actual == 4:
                    reward = 1
                elif actual == 5:
                    reward = 3
                self.reward += reward
                self.model.total_reward_this_step += reward
                # Update expectation toward actual value.
                self.expectation[cell] = (self.expectation[cell] + actual) / 2
                # (More detailed trust updates per source could be added here.)
            else:
                new_pending.append((cell, decision_round, expected))
        self.pending_rewards = new_pending

    def share_info(self):
        # Share info about the current cell.
        if self.model.environment[self.pos] >= 3:
            return None
        else:
            return (self.pos, self.model.environment[self.pos])

    def get_agent_by_id(self, agent_id):
        for agent in self.model.human_agents:
            if agent.unique_id == agent_id:
                return agent
        for agent in self.model.ai_agents:
            if agent.unique_id == agent_id:
                return agent
        return None

############################
# The AI Agent
############################

class AIAgent(Agent):
    def __init__(self, unique_id, model, strategy="exploitative", epsilon=0.05):
        Agent.__init__(self, model)
        self.unique_id = unique_id
        self.strategy = strategy
        self.epsilon = epsilon
        total_cells = model.width * model.height
        num_cells = int(total_cells * 0.1)
        self.coverage = random.sample([(x, y) for x in range(model.width) for y in range(model.height)], num_cells)
        # This dictionary stores each requester’s learned expectation.
        self.requester_expectations = {}

    def step(self):
        pass  # AI acts only when requested.
    
    def respond_info(self, requester):
        # The AI "learns" what the requester expects.
        cell = max(requester.expectation.items(), key=lambda x: x[1])[0]
        requester_belief = requester.expectation.get(cell, self.model.environment[cell])
        # Get the trust that the requester has in this AI.
        human_trust = requester.trust.get(self.unique_id, 0.5)
        learning_rate = 1 - human_trust  # Lower trust leads to a higher learning rate.
        if requester.unique_id in self.requester_expectations:
            self.requester_expectations[requester.unique_id] = (
                (1 - learning_rate) * self.requester_expectations[requester.unique_id] +
                learning_rate * requester_belief
            )
        else:
            self.requester_expectations[requester.unique_id] = requester_belief
        
        if self.strategy == "exploitative":
            return (cell, self.requester_expectations[requester.unique_id])
        else:
            if random.random() < self.epsilon:
                return (cell, self.requester_expectations[requester.unique_id])
            else:
                return (cell, self.model.environment[cell])

############################
# Running the Model and Plotting
############################

if __name__ == "__main__":
    # Run the model for a given number of steps.
    steps = 50
    model = DisasterModel()
    for i in range(steps):
        model.step()
        print(f"Step {i+1} completed.")

    # Plotting the results.
    time_steps = range(steps)
    
    # Plot 1: Average Trust (Humans vs. AI) by agent strategy.
    plt.figure(figsize=(12, 4))
    plt.subplot(1,3,1)
    plt.plot(time_steps, model.trust_exploratory_human_over_time, label="Exploratory: Trust in Humans", marker='o')
    plt.plot(time_steps, model.trust_exploratory_ai_over_time, label="Exploratory: Trust in AI", marker='o')
    plt.plot(time_steps, model.trust_exploitative_human_over_time, label="Exploitative: Trust in Humans", marker='s')
    plt.plot(time_steps, model.trust_exploitative_ai_over_time, label="Exploitative: Trust in AI", marker='s')
    plt.xlabel("Time Step")
    plt.ylabel("Average Trust")
    plt.title("Average Trust Over Time")
    plt.legend()

    # Plot 2: Total Reward per Timestep.
    plt.subplot(1,3,2)
    plt.plot(time_steps, model.reward_over_time, marker='o')
    plt.xlabel("Time Step")
    plt.ylabel("Total Reward")
    plt.title("Reward per Timestep")

    # Plot 3: Info Calls to Humans vs. AI per Timestep.
    plt.subplot(1,3,3)
    plt.plot(time_steps, model.info_calls_human_over_time, label="Calls to Humans", marker='o')
    plt.plot(time_steps, model.info_calls_ai_over_time, label="Calls to AI", marker='o')
    plt.xlabel("Time Step")
    plt.ylabel("Number of Calls")
    plt.title("Info Calls Over Time")
    plt.legend()
    
    plt.tight_layout()
    plt.show()
