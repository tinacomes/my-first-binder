import random
import math
import matplotlib.pyplot as plt
import networkx as nx

from mesa import Agent, Model
from mesa.space import MultiGrid
from mesa.time import RandomActivation

#########################################
# Disaster Model Definition (Version A: Extended)
# Based on the mechanisms on the triple filter bubble ABM
#########################################
class DisasterModel(Model):
    def __init__(self,
                 share_exploitative,          # Fraction of humans that are exploitative.
                 share_of_disaster,           # Fraction of grid cells initially affected.
                 initial_trust,               # Baseline trust for human agents.
                 initial_ai_trust,            # Baseline trust for AI agents.
                 number_of_humans,
                 share_confirming,            # Share of humans that are "confirming" (affecting info acceptance parameters).
                 disaster_dynamics=2,         # Maximum change in disaster per tick.
                 shock_probability=0.1,       # Probability that a shock occurs.
                 shock_magnitude=2,           # Maximum shock magnitude.
                 trust_update_mode="average", # "average" or "max" deviation measure.
                 exploitative_correction_factor=1.0,  # Correction factor for exploitative update.
                 width=50, height=50):
        super().__init__()
        self.share_exploitative = share_exploitative
        self.share_of_disaster = share_of_disaster
        self.base_trust = initial_trust
        self.base_ai_trust = initial_ai_trust
        self.num_humans = number_of_humans
        self.num_ai = 5
        self.share_confirming = share_confirming
        self.width = width
        self.height = height
        self.disaster_dynamics = disaster_dynamics
        self.shock_probability = shock_probability
        self.shock_magnitude = shock_magnitude
        self.trust_update_mode = trust_update_mode
        self.exploitative_correction_factor = exploitative_correction_factor

        self.grid = MultiGrid(width, height, torus=False)
        self.schedule = RandomActivation(self)
        self.tick = 0

        # Create disaster grid.
        self.disaster_grid = {}
        self.epicenter = (random.randint(0, width - 1), random.randint(0, height - 1))
        total_cells = width * height
        self.disaster_radius = math.sqrt(self.share_of_disaster * total_cells / math.pi)
        for x in range(width):
            for y in range(height):
                d = math.sqrt((x - self.epicenter[0])**2 + (y - self.epicenter[1])**2)
                if d < self.disaster_radius / 3:
                    level = 5
                elif d < 2 * self.disaster_radius / 3:
                    level = 4
                elif d < self.disaster_radius:
                    level = 3
                else:
                    level = 0
                self.disaster_grid[(x, y)] = level

        # Create a social network (Wattsâ€“Strogatz) for later use (if needed).
        self.social_network = nx.watts_strogatz_graph(self.num_humans, 4, 0.1)

        # Create human agents.
        self.humans = {}
        for i in range(self.num_humans):
            agent_type = "exploitative" if random.random() < self.share_exploitative else "exploratory"
            # Each human is also assigned an attitude type:
            # "confirming" agents will use D=0.3, delta=20; others use D=0.5, delta=5.
            a = HumanAgent(unique_id=f"H_{i}", model=self, id_num=i, agent_type=agent_type, share_confirming=self.share_confirming)
            self.humans[f"H_{i}"] = a
            self.schedule.add(a)
            x = random.randrange(width)
            y = random.randrange(height)
            self.grid.place_agent(a, (x, y))

        # Initialize trust and info_accuracy for each human.
        for i in range(self.num_humans):
            agent_id = f"H_{i}"
            agent = self.humans[agent_id]
            for j in range(self.num_humans):
                if agent_id == f"H_{j}":
                    continue
                # Baseline trust: random uniform.
                agent.trust[f"H_{j}"] = random.uniform(self.base_trust - 0.05, self.base_trust + 0.05)
                agent.info_accuracy[f"H_{j}"] = random.uniform(0.4, 0.6)
            # Boost trust for a random friend list representing 5% of the human network.
            friend_count = max(1, int(0.05 * self.num_humans))
            possible_friends = [f"H_{j}" for j in range(self.num_humans) if f"H_{j}" != agent_id]
            agent.friends = set(random.sample(possible_friends, friend_count))
            for friend_id in agent.friends:
                agent.trust[friend_id] = min(1, agent.trust[friend_id] + 0.1)
            # AI info sources.
            for k in range(self.num_ai):
                agent.trust[f"A_{k}"] = random.uniform(self.base_ai_trust - 0.1, self.base_ai_trust + 0.1)
                agent.info_accuracy[f"A_{k}"] = random.uniform(0.4, 0.7)

        # Create AI agents.
        self.ais = {}
        for k in range(self.num_ai):
            a = AIAgent(unique_id=f"A_{k}", model=self)
            self.ais[f"A_{k}"] = a
            self.schedule.add(a)
            x = random.randrange(width)
            y = random.randrange(height)
            self.grid.place_agent(a, (x, y))

        # Statistics containers.
        self.trust_data = []   # (exploitative-human, exploitative-AI, exploratory-human, exploratory-AI)
        self.calls_data = []   # (# calls to humans, # calls to AI) per group.
        self.rewards_data = [] # (total_reward_exploitative, total_reward_exploratory)

    def update_disaster(self):
        new_grid = {}
        for pos, level in self.disaster_grid.items():
            x, y = pos
            d = math.sqrt((x - self.epicenter[0])**2 + (y - self.epicenter[1])**2)
            if d < self.disaster_radius / 3:
                baseline = 5
            elif d < 2 * self.disaster_radius / 3:
                baseline = 4
            elif d < self.disaster_radius:
                baseline = 3
            else:
                baseline = 0
            diff = baseline - level
            if diff > 0:
                change = random.randint(1, int(self.disaster_dynamics))
            elif diff < 0:
                change = -random.randint(1, int(self.disaster_dynamics))
            else:
                change = 0
            new_level = level + change
            if random.random() < self.shock_probability:
                shock = random.randint(1, self.shock_magnitude)
                if random.random() < 0.5:
                    shock = -shock
                new_level += shock
            new_level = max(0, min(5, new_level))
            new_grid[pos] = new_level
        if random.random() < 0.02:
            self.epicenter = (random.randint(0, self.width - 1), random.randint(0, self.height - 1))
        self.disaster_grid = new_grid

    def step(self):
        self.update_disaster()
        self.schedule.step()
        total_reward_exploit = 0
        total_reward_explor = 0
        for agent in self.humans.values():
            agent.process_relief_actions(self.tick, self.disaster_grid)
            if agent.agent_type == "exploitative":
                total_reward_exploit += agent.total_reward
            else:
                total_reward_explor += agent.total_reward
        exp_human_trust = []
        exp_ai_trust = []
        expl_human_trust = []
        expl_ai_trust = []
        calls_exp_human = calls_exp_ai = calls_expl_human = calls_expl_ai = 0
        for agent in self.humans.values():
            human_vals = [v for key, v in agent.trust.items() if key.startswith("H_")]
            ai_vals = [v for key, v in agent.trust.items() if key.startswith("A_")]
            if agent.agent_type == "exploitative":
                if human_vals:
                    exp_human_trust.append(sum(human_vals)/len(human_vals))
                if ai_vals:
                    exp_ai_trust.append(sum(ai_vals)/len(ai_vals))
                calls_exp_human += agent.calls_human
                calls_exp_ai += agent.calls_ai
            else:
                if human_vals:
                    expl_human_trust.append(sum(human_vals)/len(human_vals))
                if ai_vals:
                    expl_ai_trust.append(sum(ai_vals)/len(ai_vals))
                calls_expl_human += agent.calls_human
                calls_expl_ai += agent.calls_ai
            agent.calls_human = 0
            agent.calls_ai = 0
            agent.total_reward = 0

        avg_exp_human_trust = sum(exp_human_trust)/len(exp_human_trust) if exp_human_trust else 0
        avg_exp_ai_trust = sum(exp_ai_trust)/len(exp_ai_trust) if exp_ai_trust else 0
        avg_expl_human_trust = sum(expl_human_trust)/len(expl_human_trust) if expl_human_trust else 0
        avg_expl_ai_trust = sum(expl_ai_trust)/len(expl_ai_trust) if expl_ai_trust else 0

        self.trust_data.append((avg_exp_human_trust, avg_exp_ai_trust, avg_expl_human_trust, avg_expl_ai_trust))
        self.calls_data.append((calls_exp_human, calls_exp_ai, calls_expl_human, calls_expl_ai))
        self.rewards_data.append((total_reward_exploit, total_reward_explor))
        self.tick += 1

#########################################
# Agent Definitions
#########################################
class HumanAgent(Agent):
    def __init__(self, unique_id, model, id_num, agent_type="exploitative", share_confirming=0.5):
        super().__init__(model)
        self.unique_id = unique_id
        self.id_num = id_num
        self.model = model
        self.agent_type = agent_type  # "exploitative" or "exploratory"
        # Assign attitude type based on share_confirming.
        if random.random() < share_confirming:
            self.attitude_type = "confirming"
            self.D = 0.3
            self.delta = 20
        else:
            self.attitude_type = "other"
            self.D = 0.5
            self.delta = 5

        self.trust = {}          # Trust values toward other agents.
        self.info_accuracy = {}  # Past accuracy.
        self.Q = {}
        self.beliefs = {(x, y): 0 for x in range(self.model.width) for y in range(self.model.height)}
        self.pending_relief = []  # Each entry: (tick, source_id, accepted_count, rejected_count)
        self.calls_human = 0
        self.calls_ai = 0
        self.total_reward = 0
        self.learning_rate = 0.1
        self.info_mode = "human"
        # Randomly select friend list: 5% of humans (at least one).
        friend_count = max(1, int(0.05 * self.model.num_humans))
        possible_friends = [f"H_{j}" for j in range(self.model.num_humans) if f"H_{j}" != self.unique_id]
        self.friends = set(random.sample(possible_friends, friend_count))

    def sense_environment(self):
        pos = self.pos
        cells = self.model.grid.get_neighborhood(pos, moore=True, include_center=True)
        for cell in cells:
            actual = self.model.disaster_grid[cell]
            if random.random() < 0.3:
                self.beliefs[cell] = max(0, min(5, actual + random.choice([-1, 1])))
            else:
                self.beliefs[cell] = actual

    def request_information(self):
        # Use friend list as candidate sources (if available).
        friend_candidates = [aid for aid in self.trust if aid in self.friends]
        if len(friend_candidates) < 2:
            candidate_set = [aid for aid in self.trust if aid.startswith("H_")]
        else:
            candidate_set = friend_candidates

        if candidate_set:
            avg_human_trust = sum([self.trust[a] for a in candidate_set]) / len(candidate_set)
            avg_human_accuracy = sum([self.info_accuracy.get(a, 0.5) for a in candidate_set]) / len(candidate_set)
        else:
            avg_human_trust, avg_human_accuracy = 0, 0.5

        ai_candidates = [aid for aid in self.trust if aid.startswith("A_")]
        if ai_candidates:
            avg_ai_trust = sum([self.trust[a] for a in ai_candidates]) / len(ai_candidates)
            avg_ai_accuracy = sum([self.info_accuracy.get(a, 0.5) for a in ai_candidates]) / len(ai_candidates)
        else:
            avg_ai_trust, avg_ai_accuracy = 0, 0.5

        if self.agent_type == "exploitative":
            human_coverage = 0.5
            ai_coverage = 0.7
            alpha, beta = 0.9, 0.0
            gamma = 0.1
        else:
            human_coverage = 0.5
            ai_coverage = 0.7
            alpha, beta = 0.3, 0.5
            gamma = 0.2
        human_utility = alpha * avg_human_trust + beta * avg_human_accuracy + gamma * human_coverage
        ai_utility = alpha * avg_ai_trust + beta * avg_ai_accuracy + gamma * ai_coverage
        self.info_mode = "ai" if ai_utility > human_utility else "human"

        responses = {}
        # We'll also record per source the number of accepted and rejected grid cells.
        if self.info_mode == "human":
            if self.agent_type == "exploitative":
                sorted_candidates = sorted(candidate_set, key=lambda a: self.trust[a])
                selected = sorted_candidates[:2] if len(sorted_candidates) >= 2 else sorted_candidates
            else:
                selected = random.sample(candidate_set, min(2, len(candidate_set)))
            for source_id in selected:
                self.calls_human += 1
                accepted = 0
                rejected = 0
                other = self.model.humans.get(source_id)
                if other is not None:
                    info = other.provide_information_full()
                    # Also get the provider's cell destruction level for potential non-response.
                    other_pos = other.pos
                    cell_level = self.model.disaster_grid[other_pos]
                    if cell_level >= 3 and random.random() < (cell_level - 2) * 0.2:
                        info = None
                    # For each grid cell in info, decide whether to adopt.
                    if info is not None:
                        for cell, reported_value in info.items():
                            d = abs(reported_value - self.beliefs[cell])
                            if d == 0:
                                P_accept = 1.0
                            else:
                                P_accept = (self.D ** self.delta) / ((d ** self.delta) + (self.D ** self.delta))
                            if random.random() < P_accept:
                                # Accept: update belief and increase trust by +0.05.
                                self.beliefs[cell] = reported_value
                                accepted += 1
                                self.trust[source_id] = min(1, self.trust[source_id] + 0.05)
                            else:
                                # Reject: no update; decrease trust by -0.05.
                                rejected += 1
                                self.trust[source_id] = max(0, self.trust[source_id] - 0.05)
                    responses[source_id] = info
                    # Store the counts for delayed update.
                    self.pending_relief.append((self.model.tick, source_id, accepted, rejected))
                else:
                    responses[source_id] = None
        else:
            if self.agent_type == "exploitative":
                selected_ai = max(ai_candidates, key=lambda a: self.trust[a]) if ai_candidates else None
            else:
                selected_ai = random.choice(ai_candidates) if ai_candidates else None
            if selected_ai is not None:
                self.calls_ai += 1
                other = self.model.ais.get(selected_ai)
                if other is not None:
                    info = other.provide_information_full(self.beliefs, trust=self.trust[selected_ai])
                    responses[selected_ai] = info
                    # For AI sources, we also update per grid cell.
                    accepted = 0
                    rejected = 0
                    for cell, reported_value in info.items():
                        d = abs(reported_value - self.beliefs[cell])
                        if d == 0:
                            P_accept = 1.0
                        else:
                            P_accept = (self.D ** self.delta) / ((d ** self.delta) + (self.D ** self.delta))
                        if random.random() < P_accept:
                            self.beliefs[cell] = reported_value
                            accepted += 1
                            self.trust[selected_ai] = min(1, self.trust[selected_ai] + 0.05)
                        else:
                            rejected += 1
                            self.trust[selected_ai] = max(0, self.trust[selected_ai] - 0.05)
                    self.pending_relief.append((self.model.tick, selected_ai, accepted, rejected))
                else:
                    responses[selected_ai] = None

    def send_relief(self):
        pos = self.pos
        if getattr(self, 'info_mode', 'human') == "ai":
            cells = self.model.grid.get_neighborhood(pos, moore=True, radius=5, include_center=True)
        else:
            cells = self.model.grid.get_neighborhood(pos, moore=True, include_center=True)
        sorted_cells = sorted(cells, key=lambda c: self.beliefs[c], reverse=True)
        selected = [c for c in sorted_cells if self.beliefs[c] >= 3][:3]
        for cell in selected:
            self.pending_relief.append((self.model.tick, None, 0, 0))
    
    def process_relief_actions(self, current_tick, disaster_grid):
        new_pending = []
        for entry in self.pending_relief:
            t, source_id, accepted, rejected = entry
            if current_tick - t >= 2:
                # Process reward for relief actions.
                # (Existing mechanism: reward = 2 if grid cell ==4; 5 if ==5; 0 otherwise)
                # Here we also update trust for exploratory agents if reward == 0.
                # For simplicity, we check one randomly selected cell among those for which info was adopted.
                # (In a more detailed model, we might track each grid cell separately.)
                sample_cell = None
                if accepted > 0:
                    sample_cell = random.choice(list(self.beliefs.keys()))
                level = disaster_grid[sample_cell] if sample_cell is not None else 0
                reward = 2 if level == 4 else (5 if level == 5 else 0)
                self.total_reward += reward
                if source_id is not None and self.agent_type == "exploratory":
                    if reward == 0 and accepted > 0:
                        # Penalize by -0.05 per accepted grid cell.
                        penalty = 0.05 * accepted
                        self.trust[source_id] = max(0, self.trust[source_id] - penalty)
                # For exploitative agents, you might apply a smaller update (if desired); here we leave them as is.
            else:
                new_pending.append(entry)
        self.pending_relief = new_pending

    def provide_information_full(self):
        pos = self.pos
        cells = self.model.grid.get_neighborhood(pos, moore=True, include_center=True)
        info = {}
        for cell in cells:
            level = self.model.disaster_grid[cell]
            if random.random() < 0.1:
                level = max(0, min(5, level + random.choice([-1, 1])))
            info[cell] = level
        return info

    def step(self):
        self.sense_environment()
        self.request_information()
        self.send_relief()

class AIAgent(Agent):
    def __init__(self, unique_id, model):
        super().__init__(model)
        self.unique_id = unique_id
        self.model = model
        self.memory = {}
        self.sensed = {}

    def sense_environment(self):
        num_cells = int(0.1 * self.model.width * self.model.height)
        self.sensed = {}
        cells = random.sample(list(self.model.disaster_grid.keys()), num_cells)
        for cell in cells:
            self.sensed[cell] = self.model.disaster_grid[cell]

    def provide_information_full(self, human_beliefs, trust):
        info = {}
        for cell, sensed_val in self.sensed.items():
            human_val = human_beliefs.get(cell, sensed_val)
            if abs(sensed_val - human_val) > 1:
                correction_factor = 1 - min(1, trust)
                corrected = round(sensed_val + correction_factor * (human_val - sensed_val))
            else:
                corrected = sensed_val
            info[cell] = corrected
        return info

    def step(self):
        self.sense_environment()

#########################################
# Main: Run Model and Plot Results
#########################################
if __name__ == "__main__":
    # Set parameters.
    share_exploitative = 0.5
    share_of_disaster = 0.2
    initial_trust = 0.7
    initial_ai_trust = 0.7
    number_of_humans = 50
    share_confirming = 0.5  # 50% of humans are "confirming"
    disaster_dynamics = 2
    shock_probability = 0.1
    shock_magnitude = 2
    trust_update_mode = "average"
    exploitative_correction_factor = 1.0
    width = 50
    height = 50

    model = DisasterModel(share_exploitative, share_of_disaster, initial_trust, initial_ai_trust,
                          number_of_humans, share_confirming, disaster_dynamics, shock_probability, shock_magnitude,
                          trust_update_mode, exploitative_correction_factor, width, height)

    ticks = 30
    for i in range(ticks):
        model.step()

    ticks_range = list(range(ticks))
    exp_human_trust = [d[0] for d in model.trust_data]
    exp_ai_trust = [d[1] for d in model.trust_data]
    expl_human_trust = [d[2] for d in model.trust_data]
    expl_ai_trust = [d[3] for d in model.trust_data]
    calls_exp_human = [d[0] for d in model.calls_data]
    calls_exp_ai = [d[1] for d in model.calls_data]
    calls_expl_human = [d[2] for d in model.calls_data]
    calls_expl_ai = [d[3] for d in model.calls_data]
    rewards_exploit = [d[0] for d in model.rewards_data]
    rewards_explor = [d[1] for d in model.rewards_data]

    plt.figure()
    plt.plot(ticks_range, rewards_exploit, label="Exploitative Total Reward")
    plt.plot(ticks_range, rewards_explor, label="Exploratory Total Reward")
    plt.xlabel("Tick")
    plt.ylabel("Total Rewards")
    plt.title("Total Rewards per Tick by Agent Type")
    plt.legend()
    plt.show()

    plt.figure()
    plt.plot(ticks_range, exp_human_trust, label="Exploitative: Human Trust")
    plt.plot(ticks_range, exp_ai_trust, label="Exploitative: AI Trust")
    plt.plot(ticks_range, expl_human_trust, label="Exploratory: Human Trust")
    plt.plot(ticks_range, expl_ai_trust, label="Exploratory: AI Trust")
    plt.xlabel("Tick")
    plt.ylabel("Average Trust")
    plt.title("Trust Evolution by Agent Type")
    plt.legend()
    plt.show()

    plt.figure()
    plt.plot(ticks_range, calls_exp_human, label="Exploitative: Calls to Humans")
    plt.plot(ticks_range, calls_exp_ai, label="Exploitative: Calls to AI")
    plt.plot(ticks_range, calls_expl_human, label="Exploratory: Calls to Humans")
    plt.plot(ticks_range, calls_expl_ai, label="Exploratory: Calls to AI")
    plt.xlabel("Tick")
    plt.ylabel("Information Requests")
    plt.title("Information Request Calls by Agent Type")
    plt.legend()
    plt.show()
